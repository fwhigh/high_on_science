{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are extremely fast to train and score, but don’t handle filter logic intelligently when segmenting populations. To segment N populations you might train N linear models, which gets operationally messy when N is large.\n",
    "\n",
    "Tree-based models are often more accurate and natively handle filter logic in a single model, but are slower to train and score (I’m thinking state of the art here: random forests and boosted decision trees).  Loosely speaking, they do this by branching on the binary segment features early, if they turn out to be important, and effectively training independent trees downstream.\n",
    "\n",
    "It is definitely possible to fake filter logic in a single linear model, but you have to engineer the feature space correctly.  This is a proof of concept illustrating how to do it.  (Hint: adding a simple binary feature indicating segment membership is not enough.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What I'm about to do\n",
    "\n",
    "I’m going to mock data from two segments (or clusters).  The predictors are continuous, and the response variable is continuous.  I’ll train each segment individually with a linear model and a tree-based model.  This will set the baselines.\n",
    "\n",
    "I’ll then try three strategies to trick the algorithms into giving me predictions from one model that are well-tuned for each segment individually.  The main idea is to manipulate the overall feature space.\n",
    "\n",
    "## Strategy 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that will be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_it(y_pred,y_actual,params):\n",
    "    '''\n",
    "    Print RMSE and any model parameters.\n",
    "    '''\n",
    "    rmse=mean_squared_error(y_pred,y_actual)\n",
    "    print \"RMSE: %f, params: %s\" % (rmse,', '.join(str(p) for p in params),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_it(xdat,ydat,xname,yname,filename=None):\n",
    "    '''\n",
    "    Scatter plot of x vs y, with 1:1 line drawn through.\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(xdat,ydat)\n",
    "    ax.set_ylabel(yname)\n",
    "    ax.set_xlabel(xname)\n",
    "    lims = [\n",
    "      np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "      np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    plt.show()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_it(x_train,y_train,x_eval,y_eval,model):\n",
    "    '''\n",
    "    Train and score.\n",
    "    '''\n",
    "    model.fit(x_train,y_train)\n",
    "    y_pred=model.predict(x_eval)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline(x_train,y_train,x_eval,y_eval,model,filename=None):\n",
    "    '''\n",
    "    Train, score, print and plot\n",
    "    '''\n",
    "    y_pred=model_it(x_train,y_train,x_eval,y_eval,model)\n",
    "    params=[]\n",
    "    if hasattr(model,'intercept_'):\n",
    "      params.append(model.intercept_)\n",
    "    if hasattr(model,'coef_'):\n",
    "      params.append(model.coef_)\n",
    "    if hasattr(model,'feature_importances_'):\n",
    "      params.append(model. feature_importances_)\n",
    "    print_it(y_pred,y_eval,params)\n",
    "    plot_it(y_eval,y_pred,'y actual','y predicted',filename=filename)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate mock data\n",
    "\n",
    "Generate 2000 random data points from two different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npts=500\n",
    "mean1=[0,-5]\n",
    "cov1=[[1,0.9],[0.9,1]]\n",
    "mean2=[0,10]\n",
    "cov2=[[1,-0.9],[-0.9,1]]\n",
    "\n",
    "x1_train,y1_train=np.random.multivariate_normal(mean1,cov1,npts).T\n",
    "x1_eval, y1_eval =np.random.multivariate_normal(mean1,cov1,npts).T\n",
    "x2_train,y2_train=np.random.multivariate_normal(mean2,cov2,npts).T\n",
    "x2_eval, y2_eval =np.random.multivariate_normal(mean2,cov2,npts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_it(np.concatenate([x1_train,x2_train]),np.concatenate([y1_train,y2_train]),'x','y','fake_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data to conform to the Scikit-learn input convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_train=x1_train[np.newaxis].T\n",
    "x1_eval =x1_eval[np.newaxis].T\n",
    "x2_train=x2_train[np.newaxis].T\n",
    "x2_eval =x2_eval[np.newaxis].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the feature sets each look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6117072 ],\n",
       "       [-0.04314303],\n",
       "       [ 0.08673768],\n",
       "       [ 1.05232617],\n",
       "       [-0.48338643],\n",
       "       [-0.92699916],\n",
       "       [-0.03961194],\n",
       "       [ 1.31193511],\n",
       "       [-1.79837495],\n",
       "       [ 1.07734479]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12114189],\n",
       "       [ 0.59167301],\n",
       "       [ 0.23436897],\n",
       "       [ 0.0035735 ],\n",
       "       [-0.36535242],\n",
       "       [ 1.78258469],\n",
       "       [-0.77359418],\n",
       "       [ 0.72499718],\n",
       "       [ 0.42903635],\n",
       "       [-0.80161925]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train[-10:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train two linear models\n",
    "\n",
    "Train a linear model on the first cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.184334, params: -5.02362107219, [ 0.90169809]\n"
     ]
    }
   ],
   "source": [
    "y1_pred=pipeline(x1_train,y1_train,x1_eval,y1_eval,LinearRegression(),'seg1_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the stats linear regression results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.02362107219 0.901698092685\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats \n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x1_train.flatten(),y1_train) \n",
    "print intercept,slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear model on the second cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.187957, params: 9.99512431438, [-0.89510196]\n"
     ]
    }
   ],
   "source": [
    "y2_pred=pipeline(x2_train,y2_train,x2_eval,y2_eval,LinearRegression(),'seg2_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the union of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.186145, params: \n"
     ]
    }
   ],
   "source": [
    "print_it(np.concatenate([y1_pred,y2_pred]),np.concatenate([y1_eval,y2_eval]),[])\n",
    "plot_it(np.concatenate([y1_eval,y2_eval]),np.concatenate([y1_pred,y2_pred]),\n",
    "        'y actual','y predicted','seg1_seg2_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train two gradient boosting regressor models\n",
    "\n",
    "Segment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.197762, params: [ 1.]\n"
     ]
    }
   ],
   "source": [
    "y1_pred=pipeline(x1_train,y1_train,x1_eval,y1_eval,GradientBoostingRegressor(),'seg1_gbr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.211919, params: [ 1.]\n"
     ]
    }
   ],
   "source": [
    "y2_pred=pipeline(x2_train,y2_train,x2_eval,y2_eval,GradientBoostingRegressor(),'seg2_gbr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the union of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.204840, params: \n"
     ]
    }
   ],
   "source": [
    "print_it(np.concatenate([y1_pred,y2_pred]),np.concatenate([y1_eval,y2_eval]),[])\n",
    "plot_it(np.concatenate([y1_eval,y2_eval]),np.concatenate([y1_pred,y2_pred]),\n",
    "        'y actual','y predicted','seg1_seg2_gbr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy 1: add a feature that flags the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape1(x1,x2,y1,y2,n):\n",
    "    '''\n",
    "    Add a binary feature that flags the segment 1 as 1, or segment 2 as 0. Append the entities together.\n",
    "    '''\n",
    "    a = np.zeros((npts*2,2))\n",
    "    a[0:npts,0:1]=x1\n",
    "    a[npts:2*npts,0:1]=x2\n",
    "    a[0:npts,1:2]=np.ones(npts)[np.newaxis].T\n",
    "    b=np.concatenate([y1,y2])\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_train,b_train=reshape1(x1_train,x2_train,y1_train,y2_train,npts)\n",
    "a_eval,b_eval=reshape1(x1_eval,x2_eval,y1_eval,y2_eval,npts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it looks like. Segment 1 is at the beginning with flag 1, segment 2 is at the end with flag 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6117072 ,  1.        ],\n",
       "       [-0.04314303,  1.        ],\n",
       "       [ 0.08673768,  1.        ],\n",
       "       [ 1.05232617,  1.        ],\n",
       "       [-0.48338643,  1.        ],\n",
       "       [-0.92699916,  1.        ],\n",
       "       [-0.03961194,  1.        ],\n",
       "       [ 1.31193511,  1.        ],\n",
       "       [-1.79837495,  1.        ],\n",
       "       [ 1.07734479,  1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35381991,  0.        ],\n",
       "       [-0.17072362,  0.        ],\n",
       "       [ 1.24057822,  0.        ],\n",
       "       [-0.26623485,  0.        ],\n",
       "       [-0.53313541,  0.        ],\n",
       "       [ 1.13699114,  0.        ],\n",
       "       [ 0.37503578,  0.        ],\n",
       "       [ 1.00696038,  0.        ],\n",
       "       [ 0.42603925,  0.        ],\n",
       "       [ 0.73331598,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train[-10:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting regressor handles this fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.200672, params: [ 0.50551446  0.49448554]\n"
     ]
    }
   ],
   "source": [
    "b_pred=pipeline(a_train,b_train,a_eval,b_eval,GradientBoostingRegressor(),'reshape1_gbr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regressor does not. Note the RMSE and the wonky figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.010466, params: 9.97744985043, [  9.60922765e-03  -1.49916779e+01]\n"
     ]
    }
   ],
   "source": [
    "b_pred=pipeline(a_train,b_train,a_eval,b_eval,LinearRegression(),'reshape1_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy 2: add two binary features that positively indicate membership in either of the two segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape2(x1,x2,y1,y2,n):\n",
    "    '''\n",
    "    For each segment, add a binary feature positively indicating segment membership.\n",
    "    '''\n",
    "    a = np.zeros((npts*2,3))\n",
    "    a[0:npts,0:1]=x1\n",
    "    a[0:npts,1:2]=np.ones(npts)[np.newaxis].T\n",
    "    a[npts:2*npts,0:1]=x2\n",
    "    a[npts:2*npts,2:3]=np.ones(npts)[np.newaxis].T\n",
    "    b=np.concatenate([y1,y2])\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_train,b_train=reshape2(x1_train,x2_train,y1_train,y2_train,npts)\n",
    "a_eval,b_eval=reshape2(x1_eval,x2_eval,y1_eval,y2_eval,npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6117072 ,  1.        ,  0.        ],\n",
       "       [-0.04314303,  1.        ,  0.        ],\n",
       "       [ 0.08673768,  1.        ,  0.        ],\n",
       "       [ 1.05232617,  1.        ,  0.        ],\n",
       "       [-0.48338643,  1.        ,  0.        ],\n",
       "       [-0.92699916,  1.        ,  0.        ],\n",
       "       [-0.03961194,  1.        ,  0.        ],\n",
       "       [ 1.31193511,  1.        ,  0.        ],\n",
       "       [-1.79837495,  1.        ,  0.        ],\n",
       "       [ 1.07734479,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35381991,  0.        ,  1.        ],\n",
       "       [-0.17072362,  0.        ,  1.        ],\n",
       "       [ 1.24057822,  0.        ,  1.        ],\n",
       "       [-0.26623485,  0.        ,  1.        ],\n",
       "       [-0.53313541,  0.        ,  1.        ],\n",
       "       [ 1.13699114,  0.        ,  1.        ],\n",
       "       [ 0.37503578,  0.        ,  1.        ],\n",
       "       [ 1.00696038,  0.        ,  1.        ],\n",
       "       [ 0.42603925,  0.        ,  1.        ],\n",
       "       [ 0.73331598,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train[-10:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.200671, params: [ 0.50544491  0.22929276  0.26526233]\n"
     ]
    }
   ],
   "source": [
    "b_pred=pipeline(a_train,b_train,a_eval,b_eval,GradientBoostingRegressor(),'reshape2_gbr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.010466, params: 0.0, [  9.60922765e-03  -5.01422806e+00   9.97744985e+00]\n"
     ]
    }
   ],
   "source": [
    "b_pred=pipeline(a_train,b_train,a_eval,b_eval,LinearRegression(fit_intercept=False),'reshape2_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy 3: add two binary features that positively indicate membership in either of the two segments, and break out the predictors to two different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape3(x1,x2,y1,y2,n):\n",
    "    '''\n",
    "    For each segment, add a binary feature flagging segment membership, and break out the continuous predictors into their own dimensions.\n",
    "    '''\n",
    "    a = np.zeros((npts*2,4))\n",
    "    a[0:npts,0:1]=x1\n",
    "    a[0:npts,1:2]=np.ones(npts)[np.newaxis].T\n",
    "    a[npts:2*npts,2:3]=x2\n",
    "    a[npts:2*npts,3:4]=np.ones(npts)[np.newaxis].T\n",
    "    b=np.concatenate([y1,y2])\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_train,b_train=reshape3(x1_train,x2_train,y1_train,y2_train,npts)\n",
    "a_eval,b_eval=reshape3(x1_eval,x2_eval,y1_eval,y2_eval,npts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6117072 ,  1.        ,  0.        ,  0.        ],\n",
       "       [-0.04314303,  1.        ,  0.        ,  0.        ],\n",
       "       [ 0.08673768,  1.        ,  0.        ,  0.        ],\n",
       "       [ 1.05232617,  1.        ,  0.        ,  0.        ],\n",
       "       [-0.48338643,  1.        ,  0.        ,  0.        ],\n",
       "       [-0.92699916,  1.        ,  0.        ,  0.        ],\n",
       "       [-0.03961194,  1.        ,  0.        ,  0.        ],\n",
       "       [ 1.31193511,  1.        ,  0.        ,  0.        ],\n",
       "       [-1.79837495,  1.        ,  0.        ,  0.        ],\n",
       "       [ 1.07734479,  1.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -1.35381991,  1.        ],\n",
       "       [ 0.        ,  0.        , -0.17072362,  1.        ],\n",
       "       [ 0.        ,  0.        ,  1.24057822,  1.        ],\n",
       "       [ 0.        ,  0.        , -0.26623485,  1.        ],\n",
       "       [ 0.        ,  0.        , -0.53313541,  1.        ],\n",
       "       [ 0.        ,  0.        ,  1.13699114,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.37503578,  1.        ],\n",
       "       [ 0.        ,  0.        ,  1.00696038,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.42603925,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.73331598,  1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train[-10:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both linear and gradient boosting regressors handle this fine, and in fact the linear regressor with default settings appears to be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.198878, params: [ 0.24338941  0.22971232  0.27884027  0.248058  ]\n"
     ]
    }
   ],
   "source": [
    "b_pred=pipeline(a_train,b_train,a_eval,b_eval,GradientBoostingRegressor(),'reshape3_gbr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.186145, params: 0.0, [ 0.90169809 -5.02362107 -0.89510196  9.99512431]\n"
     ]
    }
   ],
   "source": [
    "b_pred=pipeline(a_train,b_train,a_eval,b_eval,LinearRegression(fit_intercept=False),'reshape3_gbr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "  1. Linear and gradient boosting regressors perform equally well when trained on the two segments individually.\n",
    "  1. The linear regressor does not know how to handle binary segment-membership flags.  Gradient boosting regressors do.\n",
    "  1. You can trick linear regressors into training segments independently by (a) providing dimensions to fit independent y-intercepts to and (b) breaking the remaining predictors into their own independent dimensions.  Don't forget to turn off y-intercept fitting because you don't want that additional free parameter.\n",
    "\n",
    "There are some things to think about and additional ideas to play with.\n",
    "  * This might not work with naive regularization. \n",
    "  * Dimensionality can blow up, which would generally necessitate $L_2$ regularization even more. \n",
    "  * This approach is particularly amenable to sparse vector representations.\n",
    "  * This should work well with streaming and even mini-batch training.  In the latter case you'd need to mini-batch each segment separately. \n",
    "  * You can fake the same result by training $N$ linear models and appending the fitted coefficients and intercepts, rather than on the input data. In this case you'd still have to do the ``smash2`` preprocessing step on the vectors you're scoring, but it would be a way to use regularization without busting the whole method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
